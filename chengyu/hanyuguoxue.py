# -*- coding: utf-8 -*-
import requests
import urllib.parse
import time
import json
import re
import pymysql
from bs4 import BeautifulSoup
from neo4j import GraphDatabase

# Neo4jé…ç½®
neo4j_config = {
    "uri": "bolt://8.153.207.172:7687",
    "user": "neo4j",
    "password": "xtxzhu2u"
}

# MySQLæ•°æ®åº“é…ç½®
mysql_config = {
    "host": "8.153.207.172",
    "user": "root",
    "password": "Restart1128",
    "database": "lab_education",
    "port": 3307
}

def get_idioms_from_neo4j(limit=None):
    """
    ä»Neo4jæ•°æ®åº“è·å–æˆè¯­åˆ—è¡¨
    """
    driver = GraphDatabase.driver(neo4j_config["uri"], auth=(neo4j_config["user"], neo4j_config["password"]))
    idiom_list = []
    with driver.session() as session:
        if limit:
            query = "MATCH (n:Idiom) RETURN n.name AS name LIMIT $limit"
            result = session.run(query, limit=limit)
        else:
            query = "MATCH (n:Idiom) RETURN n.name AS name"
            result = session.run(query)
        for record in result:
            idiom_list.append(record["name"])
    driver.close()
    return idiom_list


def get_chengyu_url(chengyu):
    """
    è·å–æˆè¯­è¯¦æƒ…é¡µé¢çš„æœ€ç»ˆURL
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
    }

    search_url = f"https://www.hanyuguoxue.com/chengyu/search?words={urllib.parse.quote(chengyu)}"

    try:
        response = requests.get(search_url, headers=headers, allow_redirects=True, timeout=10)
        return response.url
    except:
        return None


def extract_chengyu_details_from_url(url):
    """
    ä»æˆè¯­è¯¦æƒ…é¡µé¢URLæå–å®Œæ•´ä¿¡æ¯
    """
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'zh-CN,zh;q=0.8,zh-TW;q=0.7,zh-HK;q=0.5,en-US;q=0.3,en;q=0.2',
    }

    try:
        response = requests.get(url, headers=headers, timeout=10)
        response.raise_for_status()
        html_content = response.text
        
        # é˜²æ­¢è¢«å°IPï¼Œæ·»åŠ å»¶æ—¶
        time.sleep(1)
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        result = {
            "url": url,
            "data": {}
        }
        
        # æå–æˆè¯­åç§°
        title_element = soup.find('h1')
        if title_element:
            result["data"]["chengyu"] = title_element.get_text().strip()
        
        # æå–æ‹¼éŸ³ä¿¡æ¯ - ä» ci-title div ä¸­çš„ pinyin div
        pinyin_element = soup.find('div', class_='ci-title')
        if pinyin_element:
            pinyin_div = pinyin_element.find('div', class_='pinyin')
            if pinyin_div:
                pinyin_spans = pinyin_div.find_all('span')
                pinyin_text = ' '.join([span.get_text().strip() for span in pinyin_spans])
                result["data"]["pinyin"] = pinyin_text
        
        # æå–åŸºæœ¬ä¿¡æ¯ - ä» ci-attrs div
        ci_attrs = soup.find('div', class_='ci-attrs')
        if ci_attrs:
            # æå–æ³¨éŸ³
            p_tags = ci_attrs.find_all('p')
            for p in p_tags:
                p_text = p.get_text().strip()
                if 'æ³¨éŸ³' in p_text:
                    # æå–æ³¨éŸ³éƒ¨åˆ†
                    zhuyin_match = re.search(r'æ³¨éŸ³[ï¼š:]\s*([^\n]+)', p_text)
                    if zhuyin_match:
                        result["data"]["zhuyin"] = zhuyin_match.group(1).strip()
                
                # æå–æ„Ÿæƒ…è‰²å½©
                if 'æ„Ÿæƒ…' in p_text:
                    emotion_link = p.find('a')
                    if emotion_link:
                        emotion_text = emotion_link.get_text().strip()
                        result["data"]["emotion"] = emotion_text
                
                # æå–è¿‘ä¹‰è¯
                if 'è¿‘ä¹‰è¯' in p_text:
                    synonyms_links = p.find_all('a')
                    synonyms = [link.get_text().strip() for link in synonyms_links]
                    result["data"]["synonyms"] = synonyms
        
        # æå–é‡Šä¹‰ - ä» ci-content div
        ci_content = soup.find('div', class_='ci-content')
        if ci_content:
            # ä¸»è¦é‡Šä¹‰
            primary_explain = ci_content.find('p', class_='explain primary')
            if primary_explain:
                # ç§»é™¤å¤åˆ¶æŒ‰é’®
                copy_button = primary_explain.find('button', class_='btn-copy')
                if copy_button:
                    copy_button.decompose()
                explanation_text = primary_explain.get_text().strip()
                result["data"]["explanation"] = explanation_text
            
            # å‡ºå¤„ã€ç”¨æ³•ã€ä¾‹å­
            ext_ps = ci_content.find_all('p', class_='ext')
            for p in ext_ps:
                p_text = p.get_text().strip()
                if 'å‡ºå¤„' in p_text:
                    source_match = re.search(r'å‡ºå¤„[ï¼š:]\s*(.+)', p_text)
                    if source_match:
                        result["data"]["source"] = source_match.group(1).strip()
                elif 'ç”¨æ³•' in p_text:
                    usage_match = re.search(r'ç”¨æ³•[ï¼š:]\s*(.+)', p_text)
                    if usage_match:
                        result["data"]["usage"] = usage_match.group(1).strip()
                elif 'ä¾‹å­' in p_text:
                    example_match = re.search(r'ä¾‹å­[ï¼š:]\s*(.+)', p_text)
                    if example_match:
                        result["data"]["example"] = example_match.group(1).strip()
        
        # æå–è‹±æ–‡ç¿»è¯‘ - ä» ci-fanyi ol
        ci_fanyi = soup.find('ol', class_='ci-fanyi')
        if ci_fanyi:
            translation_items = []
            li_elements = ci_fanyi.find_all('li')
            for li in li_elements:
                label = li.find('label')
                if label:
                    language = label.get_text().strip()
                    # ç§»é™¤labelå…ƒç´ ï¼Œè·å–çº¯ç¿»è¯‘æ–‡æœ¬
                    label.decompose()
                    translation_text = li.get_text().strip()
                    translation_items.append(f"{language}: {translation_text}")
            result["data"]["translation"] = '; '.join(translation_items)
        
        # æå–ç»“æ„ä¿¡æ¯ - ä» ci-cards ul
        ci_cards = soup.find('div', class_='ci-cards')
        if ci_cards:
            structure_info = {}
            li_elements = ci_cards.find_all('li')
            for li in li_elements:
                span = li.find('span')
                if span:
                    key = span.get_text().strip()
                    link = li.find('a')
                    if link:
                        value = link.get_text().strip()
                        structure_info[key] = value
            result["data"]["structure"] = structure_info
        
        return result
        
    except Exception as e:
        return {
            "url": url,
            "error": str(e)
        }


def get_database_connection():
    """
    è·å–MySQLæ•°æ®åº“è¿æ¥
    """
    try:
        connection = pymysql.connect(
            host=mysql_config["host"],
            user=mysql_config["user"],
            password=mysql_config["password"],
            database=mysql_config["database"],
            port=mysql_config["port"],
            charset="utf8mb4",
            cursorclass=pymysql.cursors.DictCursor
        )
        return connection
    except Exception as e:
        print(f"æ•°æ®åº“è¿æ¥å¤±è´¥: {e}")
        return None





def save_chengyu_to_db(chengyu_data):
    """
    å°†æˆè¯­æ•°æ®ä¿å­˜åˆ°æ•°æ®åº“
    """
    connection = get_database_connection()
    if not connection:
        return False

    try:
        cursor = connection.cursor()
        
        # æå–æˆè¯­åç§°
        chengyu = ""
        if 'data' in chengyu_data and 'chengyu' in chengyu_data['data']:
            chengyu = chengyu_data['data']['chengyu']
        
        # å¦‚æœæœ‰é”™è¯¯ä¿¡æ¯ï¼Œä¿å­˜é”™è¯¯è®°å½•
        if 'error' in chengyu_data:
            sql = """
            INSERT INTO hanyuguoxue_chengyu
            (chengyu, url, error)
            VALUES (%s, %s, %s)
            ON DUPLICATE KEY UPDATE
            url = VALUES(url),
            error = VALUES(error),
            updated_at = CURRENT_TIMESTAMP
            """
            cursor.execute(sql, (
                chengyu,
                chengyu_data.get('url', ''),
                chengyu_data['error']
            ))
        else:
            # ä¿å­˜å®Œæ•´æ•°æ®
            data = chengyu_data.get('data', {})
            sql = """
            INSERT INTO hanyuguoxue_chengyu
            (chengyu, url, pinyin, zhuyin, fanti, emotion, explanation, 
             source, usage, example, synonyms, antonyms, translation)
            VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
            ON DUPLICATE KEY UPDATE
            url = VALUES(url),
            pinyin = VALUES(pinyin),
            zhuyin = VALUES(zhuyin),
            fanti = VALUES(fanti),
            emotion = VALUES(emotion),
            explanation = VALUES(explanation),
            source = VALUES(source),
            usage = VALUES(usage),
            example = VALUES(example),
            synonyms = VALUES(synonyms),
            antonyms = VALUES(antonyms),
            translation = VALUES(translation),
            updated_at = CURRENT_TIMESTAMP
            """
            cursor.execute(sql, (
                chengyu,
                chengyu_data.get('url', ''),
                data.get('pinyin', ''),
                data.get('zhuyin', ''),
                data.get('fanti', ''),
                data.get('emotion', ''),
                data.get('explanation', ''),
                data.get('source', ''),
                data.get('usage', ''),
                data.get('example', ''),
                json.dumps(data.get('synonyms', []), ensure_ascii=False),
                json.dumps(data.get('antonyms', []), ensure_ascii=False),
                data.get('translation', '')
            ))
        
        connection.commit()
        return True
        
    except Exception as e:
        print(f"ä¿å­˜æˆè¯­æ•°æ®åˆ°æ•°æ®åº“å¤±è´¥: {e}")
        connection.rollback()
        return False
    finally:
        connection.close()


def crawl_all_chengyu(limit=None, start_index=0):
    """
    æ‰¹é‡çˆ¬å–æ‰€æœ‰æˆè¯­æ•°æ®
    Args:
        limit: é™åˆ¶çˆ¬å–æ•°é‡ï¼ŒNoneè¡¨ç¤ºçˆ¬å–å…¨éƒ¨
        start_index: å¼€å§‹ç´¢å¼•ï¼Œç”¨äºæ–­ç‚¹ç»­çˆ¬
    """
    # æ³¨æ„ï¼šä½¿ç”¨å‰è¯·å…ˆè¿è¡Œ create_table.py åˆ›å»ºæ•°æ®è¡¨
    
    # è·å–æˆè¯­åˆ—è¡¨
    print("æ­£åœ¨ä»Neo4jè·å–æˆè¯­åˆ—è¡¨...")
    chengyu_list = get_idioms_from_neo4j(limit=None)  # è·å–æ‰€æœ‰æˆè¯­
    
    if not chengyu_list:
        print("æœªè·å–åˆ°æˆè¯­åˆ—è¡¨")
        return
    
    total_chengyu = len(chengyu_list)
    print(f"å…±è·å–åˆ° {total_chengyu} ä¸ªæˆè¯­")
    
    # åº”ç”¨é™åˆ¶å’Œèµ·å§‹ç´¢å¼•
    if start_index >= total_chengyu:
        print(f"èµ·å§‹ç´¢å¼• {start_index} è¶…å‡ºèŒƒå›´ï¼Œæ€»æˆè¯­æ•°: {total_chengyu}")
        return
    
    end_index = total_chengyu
    if limit:
        end_index = min(start_index + limit, total_chengyu)
    
    chengyu_list = chengyu_list[start_index:end_index]
    
    successful_crawls = 0
    failed_crawls = 0
    
    print(f"å¼€å§‹çˆ¬å–æˆè¯­ï¼ŒèŒƒå›´: {start_index+1}-{end_index}/{total_chengyu}")
    print("=" * 60)
    
    for i, chengyu in enumerate(chengyu_list, start=start_index + 1):
        try:
            print(f"ã€{i:4d}/{end_index}ã€‘æ­£åœ¨çˆ¬å–: {chengyu}")
            
            # è·å–æˆè¯­è¯¦æƒ…é¡µé¢URL
            url = get_chengyu_url(chengyu)
            if not url:
                print(f"  âŒ æ— æ³•è·å– {chengyu} çš„è¯¦æƒ…é¡µé¢URL")
                failed_crawls += 1
                continue
            
            # æå–æˆè¯­è¯¦æƒ…
            chengyu_data = extract_chengyu_details_from_url(url)
            
            # ä¿å­˜åˆ°æ•°æ®åº“
            if save_chengyu_to_db(chengyu_data):
                successful_crawls += 1
                print(f"  âœ… æˆåŠŸä¿å­˜: {chengyu}")
                
                # æ˜¾ç¤ºéƒ¨åˆ†ä¿¡æ¯
                if 'data' in chengyu_data:
                    data = chengyu_data['data']
                    if 'pinyin' in data:
                        print(f"    æ‹¼éŸ³: {data['pinyin']}")
                    if 'emotion' in data:
                        print(f"    æ„Ÿæƒ…: {data['emotion']}")
                    if 'synonyms' in data and data['synonyms']:
                        print(f"    è¿‘ä¹‰è¯: {', '.join(data['synonyms'][:3])}{'...' if len(data['synonyms']) > 3 else ''}")
                    if 'antonyms' in data and data['antonyms']:
                        print(f"    åä¹‰è¯: {', '.join(data['antonyms'][:3])}{'...' if len(data['antonyms']) > 3 else ''}")
            else:
                failed_crawls += 1
                print(f"  âŒ ä¿å­˜å¤±è´¥: {chengyu}")
            
            # æ¯å¤„ç†10ä¸ªæˆè¯­æ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
            if i % 10 == 0:
                progress = i / end_index * 100
                print(f"è¿›åº¦: {progress:.1f}% (æˆåŠŸ: {successful_crawls}, å¤±è´¥: {failed_crawls})")
            
        except Exception as e:
            failed_crawls += 1
            print(f"  âŒ çˆ¬å– {chengyu} æ—¶å‘ç”Ÿé”™è¯¯: {e}")
            continue
    
    print("=" * 60)
    print(f"çˆ¬å–å®Œæˆï¼")
    print(f"å¤„ç†æˆè¯­æ•°: {end_index - start_index}")
    print(f"æˆåŠŸçˆ¬å–: {successful_crawls}")
    print(f"å¤±è´¥çˆ¬å–: {failed_crawls}")
    print(f"æˆåŠŸç‡: {successful_crawls/(successful_crawls+failed_crawls)*100:.2f}%" if (successful_crawls+failed_crawls) > 0 else "0%")

if __name__ == "__main__":
    import sys
    
    if len(sys.argv) < 2:
        print("ç”¨æ³•: python hanyuguoxue.py [å‘½ä»¤] [å‚æ•°]")
        print("å‘½ä»¤:")
        print("  crawl [limit] [start_index] - çˆ¬å–æˆè¯­æ•°æ®")
        print("ç¤ºä¾‹:")
        print("  python hanyuguoxue.py crawl 10 0    # çˆ¬å–å‰10ä¸ªæˆè¯­")
        print("  python hanyyuoxue.py crawl 100 50   # ä»ç¬¬51ä¸ªå¼€å§‹çˆ¬å–100ä¸ªæˆè¯­")
        print("  python hanyuguoxue.py crawl          # çˆ¬å–æ‰€æœ‰æˆè¯­")
        sys.exit(1)
    
    command = sys.argv[1]
    
    if command == "crawl":
        limit = None
        start_index = 0
        
        if len(sys.argv) >= 3:
            limit = int(sys.argv[2])
        
        if len(sys.argv) >= 4:
            start_index = int(sys.argv[3])
        
        print(f"ğŸš€ å¼€å§‹çˆ¬å–æˆè¯­æ•°æ®...")
        print(f"é™åˆ¶æ•°é‡: {limit if limit else 'å…¨éƒ¨'}")
        print(f"èµ·å§‹ç´¢å¼•: {start_index}")
        print("="*60)
        
        crawl_all_chengyu(limit=limit, start_index=start_index)
    else:
        print(f"âŒ æœªçŸ¥å‘½ä»¤: {command}")
        sys.exit(1)